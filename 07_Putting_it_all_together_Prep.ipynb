{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "515d4dbb-e76f-4c51-ae48-a490bd6f6590",
   "metadata": {},
   "source": [
    "# Putting it all together - Preparing the Environment\n",
    "\n",
    "Welcome back! In the previous sections, you've explored the foundational building blocks of developing with Llama Stack: understanding basic agents, delving into the power of ReAct agents, and seeing how the Model Context Protocol (MCP) allows seamless integration of external services as tools.\n",
    "\n",
    "Now, it's time to bring these concepts together and prepare our environment for building a more sophisticated agent capable of interacting with multiple external services and leveraging internal knowledge. This section is dedicated to setting the stage, ensuring all the necessary components are up and running and configured correctly within Llama Stack.\n",
    "\n",
    "By the end of this preparatory section, you will have:\n",
    "\n",
    "* Understood the setup required for a multi-tool, RAG-enabled agent.\n",
    "* Launched and verified the operation of multiple MCP servers exposing diverse functionalities.\n",
    "* Configured your Llama Stack client to interact with these new tools.\n",
    "* Registered the MCP servers as discoverable toolgroups within Llama Stack.\n",
    "* Created and populated a Vector Database, essential for providing our agent with domain-specific knowledge via Retrieval Augmented Generation (RAG).\n",
    "\n",
    "Think of this section as gathering and organizing all the ingredients and setting up your workspace before you start cooking a complex and exciting dish! Let's get everything ready so we can build something truly powerful in the next section.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c58715-74f5-4c41-af9b-2e08f537746f",
   "metadata": {},
   "source": [
    "### Setting up our Environment\n",
    "\n",
    "Before we dive into the exciting part of building our multi-tool agent, let's make sure our environment is set up correctly. We'll start by importing the essential libraries and configuring our connection to the Llama Stack server, just like we did in previous labs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f8d7728-a109-43c2-bb8b-96d8f0c11d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip Python Prerequisites installed succesfuly\n",
      "--- Available models: ---\n",
      "meta-llama/Llama-3.2-3B-Instruct - ollama - llama3.2:3b-instruct-fp16\n",
      "all-MiniLM-L6-v2 - ollama - all-minilm:latest\n",
      "granite3.2:8b - ollama - granite3.2:8b\n",
      "Selected model (from allowed list): meta-llama/Llama-3.2-3B-Instruct\n"
     ]
    }
   ],
   "source": [
    "!pip install -U llama-stack-client==0.2.7 dotenv > /dev/null 2>&1 && echo \"pip Python Prerequisites installed succesfuly\"\n",
    "import os\n",
    "from src.utils import step_printer\n",
    "from termcolor import cprint\n",
    "import uuid\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "\n",
    "\n",
    "\n",
    "stream=False \n",
    "allowed_models_list=[\"meta-llama/Llama-3.2-3B-Instruct\"]\n",
    "\n",
    "LLAMA_STACK_SERVER='http://localhost:8321'\n",
    "client = LlamaStackClient(\n",
    "    base_url=LLAMA_STACK_SERVER,\n",
    ")\n",
    "\n",
    "selected_model = None\n",
    "models = client.models.list()\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"{m.identifier} - {m.provider_id} - {m.provider_resource_id}\")\n",
    "    # Check if the model identifier contains any of the allowed substrings\n",
    "    if any(substring in m.identifier for substring in allowed_models_list):\n",
    "        # Only set selected_model if it hasn't been set yet\n",
    "        if selected_model is None:\n",
    "            selected_model = m.identifier\n",
    "           \n",
    "if selected_model is None:\n",
    "    print(\"No allowed model found in the list.\")\n",
    "\n",
    "\n",
    "print(f\"Selected model (from allowed list): {selected_model}\")\n",
    "            # Removed the break here to show all available models, but the selection logic remains picking the first one\n",
    "SELECTED_MODEL = selected_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc7d613-ed4d-43d3-95a5-2f4102b808ef",
   "metadata": {},
   "source": [
    "### Launching our External Services (MCP Servers)\n",
    "\n",
    "To create a truly capable agent, we need to connect it to the outside world! We'll be using a few specialized external services, exposed via the Model Context Protocol (MCP), to give our agent powers like checking the weather, finding locations, and accessing specific park information.\n",
    "\n",
    "Run the following commands in your terminal to start these essential MCP servers in the background:\n",
    "\n",
    "* `mcp-weather`: Adds real-time weather capabilities.\n",
    "* `mcp-googlemaps`: Connects to Google Maps for location and direction data.\n",
    "* `mcp-parks-info`: Offers RAG-based information about our fictional parks using RAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76a3e1b4-6d77-4384-aff3-5033bb34b60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "time=\"2025-05-17T18:11:30Z\" level=warning msg=\"StopSignal SIGTERM failed to stop container mcp-weather in 10 seconds, resorting to SIGKILL\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5885c6b7f3ff2e1714a65e5bcbd433f54a8c7e827da75b5408f840e04175e968\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to pull quay.dev.demo.redhat.com/rhdp/mcp-googlemaps-sse:latest...\n",
      "Getting image source signatures\n",
      "Copying blob sha256:9072e2b43e428f5b955a494f416c2f7ce1c7895cabe222f3ab58f15d5936adcb\n",
      "Copying blob sha256:af70d51bc91f4dfec8899951d2ec3b55a5a0ac08279db44e5a797a1028220413\n",
      "Copying blob sha256:b0f6f1c319a1570f67352d490370f0aeb5c0e67a087baf2d5f301ad51ec18858\n",
      "Copying blob sha256:4f361b47bbd6d447cf938eeb7945ff5851832edf4daa196acfcf963fd2377339\n",
      "Copying blob sha256:60f51d565ac390f55b710d0403051fbdc8c67dcd340a3435543be93637630843\n",
      "Copying blob sha256:043debd7ea09cd971f3eae8dc4a62ccc5f581617ece07246fd080b0f62d56356\n",
      "Copying blob sha256:26d756338f506b2f00ec798097134f931ddefd05edfe7dd957b6acedd0539fa6\n",
      "Copying blob sha256:4b889521ebfd129dd84cc5c9295deaf663ca3d611ba1f4168aae2e5365c5c6bb\n",
      "Copying blob sha256:a391a831567b9a1af33465a7054129da83dea008c59b4f24d9160162c0bf8595\n",
      "Copying blob sha256:f29bf23b693f430dfb25292d9e0b29e48d1d524eb1b775755f826f494802884e\n",
      "Copying config sha256:8f67da8740421a982b2ee3af4215a50ce918ba666ee3905ebc9b459a458f9343\n",
      "Writing manifest to image destination\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4d5933bac1a3ff8724736a2e75fe8a4244455ab67e70e5b53c2ab0736d6b60fa\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trying to pull quay.dev.demo.redhat.com/rhdp/mcp-parks-info:latest...\n",
      "Getting image source signatures\n",
      "Copying blob sha256:d2356d971238ca958d4fbdaf64c3548328e1a1c1d27663aaadd8dc8eccc78a00\n",
      "Copying blob sha256:a8d0f3a3203486c1cc2932289a0b552768e6e3c52076b384c3e7731e77fe39ae\n",
      "Copying blob sha256:08f144d6f626c0e3bdc90940ee48538bf87b15bfc122b93f96a5a2f1f77efd12\n",
      "Copying blob sha256:3d04dbd71e156befabe4a49dbd96e04907e64132ed7043ed926d3f3724eab73a\n",
      "Copying blob sha256:24626d2b50367d914dc5b504c0d949ddc1b00e25e45ac39786bbeea7a68f7045\n",
      "Copying blob sha256:7f48b8fefae7246ca5e3650ea24027cfb03d5ccda5146f6aedb5b8b2e4d356fe\n",
      "Copying config sha256:cf772fa3de2a10d8b9d6ff362115252d6b129004180029bc00839ea422de5cc9\n",
      "Writing manifest to image destination\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84285ed3fb4d3aaadcd8e25438372c58be874de27a6230fb8b2813d871b34d82\n",
      "CONTAINER ID  IMAGE                                                    COMMAND               CREATED                 STATUS                 PORTS               NAMES\n",
      "e5dcde41db91  docker.io/llamastack/distribution-ollama:0.2.7           --port 8321 --env...  About an hour ago       Up About an hour                           llamastack-ollama\n",
      "5885c6b7f3ff  quay.dev.demo.redhat.com/rhdp/mcp-weather:latest         --port 8005           14 seconds ago          Up 15 seconds          8000/tcp, 8080/tcp  mcp-weather\n",
      "4d5933bac1a3  quay.dev.demo.redhat.com/rhdp/mcp-googlemaps-sse:latest                        8 seconds ago           Up 9 seconds                               mcp-googlemaps-sse\n",
      "84285ed3fb4d  quay.dev.demo.redhat.com/rhdp/mcp-parks-info:latest                            Less than a second ago  Up Less than a second  8000/tcp, 8080/tcp  mcp-parks-info\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "source ~/.bashrc\n",
    "podman run -d --replace --name mcp-weather --network=host quay.dev.demo.redhat.com/rhdp/mcp-weather:latest --port 8005 \n",
    "podman run -d --replace --name mcp-googlemaps-sse --network=host -e GOOGLE_MAPS_API_KEY=$GOOGLE_MAPS_API_KEY -e MCP_PORT=8006 quay.dev.demo.redhat.com/rhdp/mcp-googlemaps-sse:latest\n",
    "podman run -d --replace --name mcp-parks-info --network=host -e PORT=8007 quay.dev.demo.redhat.com/rhdp/mcp-parks-info:latest\n",
    "\n",
    "podman ps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7efd52-e1f8-4e2a-b76d-b15e2895b184",
   "metadata": {},
   "source": [
    "### Verify Servers are Running\n",
    "\n",
    "It's always a good practice to quickly check if our external services have started successfully. We can do this by attempting to connect to their status endpoints. Run the code below to verify they are live and ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20113860-d39e-4974-b660-10825790179b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing MCP-WEATHER\n",
      "event: endpoint\n",
      "data: /messages/?session_id=a8fc55f4985b42acb62ff909a8117fe0\n",
      "\n",
      "Testing MCP-GOOGLEMAPS-sse\n",
      "event: endpoint\n",
      "data: /message?sessionId=09dce9dc-3235-4e55-a9b8-654994114e6a\n",
      "\n",
      "Testing mcp-parks-info\n",
      "event: endpoint\n",
      "data: /messages/?session_id=8e44a69a68e84310bdf8d19ee24cf04a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!echo Testing MCP-WEATHER ; curl --max-time 1 http://localhost:8005/sse 2>/dev/null\n",
    "!echo Testing MCP-GOOGLEMAPS-sse ; curl --max-time 1 http://localhost:8006/sse 2>/dev/null \n",
    "!echo Testing mcp-parks-info ; curl --max-time 1 http://localhost:8007/sse 2>/dev/null \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "465e9167-1613-45df-9819-f1e7bcc91905",
   "metadata": {},
   "source": [
    "### Connect to Llama Stack and Select Our Model\n",
    "\n",
    "Now that our external services are running, let's establish the connection from our notebook to the Llama Stack server and select the powerful language model our agent will use for reasoning and action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9bf6a1e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pip Python Prerequisites installed succesfuly\n",
      "--- Available models: ---\n",
      "meta-llama/Llama-3.2-3B-Instruct - ollama - llama3.2:3b-instruct-fp16\n",
      "all-MiniLM-L6-v2 - ollama - all-minilm:latest\n",
      "granite3.2:8b - ollama - granite3.2:8b\n",
      "Selected model (from allowed list): granite3.2:8b\n"
     ]
    }
   ],
   "source": [
    "!pip install -U llama-stack-client==0.2.7 dotenv > /dev/null 2>&1 && echo \"pip Python Prerequisites installed succesfuly\"\n",
    "\n",
    "import os\n",
    "from src.utils import step_printer\n",
    "from termcolor import cprint\n",
    "import uuid\n",
    "\n",
    "from llama_stack_client import LlamaStackClient\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "\n",
    "\n",
    "\n",
    "stream=False \n",
    "allowed_models_list=[\"granite3.2:8b\"]\n",
    "#allowed_models_list=[\"meta-llama/Llama-3.2-3B-Instruct\"]\n",
    "\n",
    "LLAMA_STACK_SERVER='http://localhost:8321'\n",
    "client = LlamaStackClient(\n",
    "    base_url=LLAMA_STACK_SERVER,\n",
    ")\n",
    "\n",
    "selected_model = None\n",
    "models = client.models.list()\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"{m.identifier} - {m.provider_id} - {m.provider_resource_id}\")\n",
    "    # Check if the model identifier contains any of the allowed substrings\n",
    "    if any(substring in m.identifier for substring in allowed_models_list):\n",
    "        # Only set selected_model if it hasn't been set yet\n",
    "        if selected_model is None:\n",
    "            selected_model = m.identifier\n",
    "           \n",
    "if selected_model is None:\n",
    "    print(\"No allowed model found in the list.\")\n",
    "\n",
    "\n",
    "print(f\"Selected model (from allowed list): {selected_model}\")\n",
    "            # Removed the break here to show all available models, but the selection logic remains picking the first one\n",
    "SELECTED_MODEL = selected_model\n",
    "\n",
    "vector_db_id = \"Our_Parks_DB\"\n",
    "query_config = {\n",
    "    \"query_generator_config\": {\n",
    "        \"type\": \"default\",\n",
    "        \"separator\": \" \"\n",
    "    },\n",
    "    \"max_tokens_in_context\": 300,\n",
    "    \"max_chunks\": 2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a9de5c-6f9e-43da-aac3-3b43bc62fb50",
   "metadata": {},
   "source": [
    "### Making External Services Available as Tools\n",
    "\n",
    "This is where the magic happens! We need to tell Llama Stack about the external services we just started. By registering them as 'toolgroups', Llama Stack understands their capabilities and can make them available for our agent to discover and use in its problem-solving process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d23404fc-4496-4d65-8f6c-24ead07cb8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::mcp-weather\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":\"http://localhost:8005/sse\"},\n",
    "    )\n",
    "\n",
    "client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::mcp-googlemaps\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":\"http://localhost:8006/sse\"},\n",
    "    )\n",
    "client.toolgroups.register(\n",
    "        toolgroup_id=\"mcp::mcp-parks-info\",\n",
    "        provider_id=\"model-context-protocol\",\n",
    "        mcp_endpoint={\"uri\":\"http://localhost:8007/sse\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7811cc-2a4d-4db9-89f2-06a6170ed072",
   "metadata": {},
   "source": [
    "## Building Our Agent's Knowledge Base (RAG)\n",
    "\n",
    "A truly smart agent can also access specific, domain-level knowledge beyond its initial training. We'll achieve this using Retrieval Augmented Generation (RAG). The first step is to create and populate a Vector Database with the information our agent might need – in this case, details about our parks!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca62f196-4166-40d3-a819-3b37cf896fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unregistering vector database: test_vector_db_33e67287-a7ac-4845-88e3-0c8895b6d28a\n",
      "Created 5 documents from local files into Our_Parks_DB\n"
     ]
    }
   ],
   "source": [
    "#Unregistering vector database in case we are running this over and over again as part of a learning experience\n",
    "\n",
    "for vector_db_id in client.vector_dbs.list():\n",
    "    print(f\"Unregistering vector database: {vector_db_id.identifier}\")\n",
    "    client.vector_dbs.unregister(vector_db_id=vector_db_id.identifier)\n",
    "\n",
    "# Select a VectorDB provider from availalbe providers\n",
    "providers = client.providers.list()\n",
    "vector_providers = []\n",
    "for provider in client.providers.list():\n",
    "    if provider.api == \"vector_io\":\n",
    "        #print(f\"Found VectorDB provider: {provider.provider_id}\\n\")  # Simple print\n",
    "        vector_providers.append(provider)\n",
    "\n",
    "# In this example, we only have one provider, but on other server we might have many. here, we simply select the first one.\n",
    "selected_vector_provider = vector_providers[0]\n",
    "\n",
    "# register our DB\n",
    "vector_db_id = \"Our_Parks_DB\"\n",
    "client.vector_dbs.register(\n",
    "    vector_db_id=vector_db_id,\n",
    "    embedding_model=\"all-MiniLM-L6-v2\",\n",
    "    embedding_dimension=384,\n",
    "    provider_id=selected_vector_provider.provider_id,\n",
    ")\n",
    "\n",
    "# Injest documents from local directory (this is so you can test inserting changes easily)\n",
    "from llama_stack_client.types import Document\n",
    "\n",
    "files = [\n",
    "    \"Azure_Mongrove_Wilderness.md\",\n",
    "    \"Crimson_Basin.md\",\n",
    "     \"Granite_Spire.md\",\n",
    "     \"Obsidian_Rainforest.md\",\n",
    "     \"Prismatic_Painted_Prairie.md\",\n",
    "]\n",
    "\n",
    "document_dirctory=\"assets/Parks\"\n",
    "\n",
    "# Read documents into the \"documents\" array\n",
    "documents = [\n",
    "    Document(\n",
    "        document_id=f\"num-{i}\",\n",
    "        content=open(os.path.join(document_dirctory, file), 'r', encoding='utf-8').read(),\n",
    "        mime_type=\"text/plain\",\n",
    "        metadata={},\n",
    "    )\n",
    "    for i, file in enumerate(files)\n",
    "]\n",
    "\n",
    "# Insert the documents into the vectorDB\n",
    "client.tool_runtime.rag_tool.insert(\n",
    "    documents=documents,\n",
    "    vector_db_id=vector_db_id,\n",
    "    chunk_size_in_tokens=300,\n",
    ")\n",
    "\n",
    "print(f\"Created {len(documents)} documents from local files into {vector_db_id}\")\n",
    "\n",
    "\n",
    "#print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a656d8a7-e759-4bb2-9fe1-afac153960c4",
   "metadata": {},
   "source": [
    "# Summary: Putting it all together - Preparing the Environment\n",
    "\n",
    "In this preparatory section, you've successfully laid the groundwork for building a more advanced agent within the Llama Stack framework. You've taken the essential steps to integrate multiple external services and prepare a knowledge base for your agent to utilize.\n",
    "\n",
    "You have accomplished the following key tasks:\n",
    "\n",
    "* **Deployed Multiple MCP Servers:** You initiated and confirmed the operation of several MCP-enabled containers (`mcp-weather`, `mcp-googlemaps`, `mcp-parks-info`), demonstrating how Llama Stack can connect to a variety of external functionalities.\n",
    "* **Configured the Llama Stack Client:** You set up your Python client, specifying the server address and selecting the appropriate language model that will power your agent.\n",
    "* **Registered External Tools:** You registered the different MCP servers as toolgroups within Llama Stack, making the functions they expose available for discovery and use by agents.\n",
    "* **Established a Knowledge Base:** You created and populated a Vector Database with relevant documents, setting up the critical component for Retrieval Augmented Generation (RAG) to give your agent access to specific information.\n",
    "\n",
    "By completing these steps, your environment is now ready for you to build and experiment with an agent that can combine reasoning, acting with diverse tools (both external services via MCP and internal RAG), and leveraging a custom knowledge base. This is a significant step towards creating more intelligent and capable AI applications with Llama Stack!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08368a3-2cfd-47e7-bbf1-f77d550222e1",
   "metadata": {},
   "source": [
    "### Cleaning Up (Optional)\n",
    "\n",
    "If you need to stop and remove the MCP server containers later, you can use these commands. This is helpful for resetting your environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33be14de-6a8d-41ac-96be-a9f7adb38eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "DON\"T RUN THIS IF YOU DON\"T MEAN TO, IT WILL DELETE YOUR DATABASES\n",
    "\n",
    "toolgroups_to_unregister = [\n",
    "    \"mcp::mcp-weather\",\n",
    "    \"mcp::mcp-googlemaps\",\n",
    "    \"mcp::mcp-parks-info\",\n",
    "]\n",
    "\n",
    "for toolgroup_id in toolgroups_to_unregister:\n",
    "    try:\n",
    "        print(f\"Attempting to unregister toolgroup: {toolgroup_id}\")\n",
    "        client.toolgroups.unregister(toolgroup_id=toolgroup_id)\n",
    "        print(f\"Successfully unregistered toolgroup: {toolgroup_id}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to unregister toolgroup {toolgroup_id}. Error: {e}\")\n",
    "\n",
    "print(\"\\nFinished attempting to unregister toolgroups.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f1c70-ce6c-4dad-a289-36cd88a98ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "THIS WILL DELETE ALL OF YOUR DATABASES ONLY DO THIS IF YOU MEAN TOOOOO \n",
    "#Unregister all vector databases (THIS IS FOR DEBUG NOT FOR LAB)\n",
    "for vector_db_id in client.vector_dbs.list():\n",
    "    print(f\"Unregistering vector database: {vector_db_id.identifier}\")\n",
    "    client.vector_dbs.unregister(vector_db_id=vector_db_id.identifier)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
