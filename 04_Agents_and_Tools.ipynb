{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f75a76c-cb61-4c32-a334-a5bcd41f4e7d",
   "metadata": {},
   "source": [
    "# Agents and Tools\n",
    "\n",
    "In this section of the lab, we will review some agent concepts and see how agents can interact the basic tools\n",
    "\n",
    "- Define a basic agent, using the `builtin::websearch` tool from Llama Stack\n",
    "- Get familiar with concepts like \"Turns\" and \"Steps\" and \"Tool_Calling\" that the agent takes\n",
    "- Run the agent to return web results\n",
    "- Optional: experience a prompt chaining example\n",
    "\n",
    "> **Note:**\n",
    ">This section is intended only as an introduction to Agent concepts and language, and we will get into more exciting work in the next modules. If you are familiar with these concepts, feel free to skip this section.\n",
    "> \n",
    "\n",
    "\n",
    "> **Note:**\n",
    "> For the sake of simplicity and time, we will skip through some of the python basics we went through in previous modules and treat this more like a nomral python application \n",
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089d285-a329-4f61-9edc-3dff39015877",
   "metadata": {},
   "source": [
    "### Install Python Prerequisist\n",
    "\n",
    "As always, let's start by installing the Python Libraries we neeed, we will add some additional libraries to support some of our tools. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7268ad4-bda8-427c-9320-3c836e861887",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install -U llama-stack-client==0.2.5 dotenv geocoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f62bc6b-c7a9-40a5-93ff-a0b8dfd75851",
   "metadata": {},
   "source": [
    "### Define the LLamastack server and Model\n",
    "\n",
    "Let's define the libraries we need and our basic Llama Stack configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a729f5-6954-4ffc-844c-5b758204a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# for communication with Llama Stack\n",
    "from llama_stack_client import LlamaStackClient\n",
    "\n",
    "# These libraries are just here to print the results from the agent in a more human-readable way \n",
    "from src.utils import step_printer\n",
    "from termcolor import cprint\n",
    "import uuid\n",
    "from llama_stack_client.lib.agents.event_logger import EventLogger\n",
    "stream=False ## Defaulting to False, you can change this throughout the section to \"True\" if you wanted to see the output in another format (Using EventLogger)\n",
    "\n",
    "# for our lab, we will just define our variables manualy here, in a regular application, this would be ready directly from the local .env file and we would comment these lines out\n",
    "os.environ['LLAMA_STACK_SERVER'] = 'http://localhost:8321'\n",
    "\n",
    "# We will be using the Tavily web search service (docs.tavily.com/)\n",
    "tavily_search_api_key='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'\n",
    "provider_data = {\"tavily_search_api_key\": tavily_search_api_key}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d2008b-85be-4be6-9e05-82cc25aac275",
   "metadata": {},
   "source": [
    "> **Note:**\n",
    ">When running this code in a regular Python application, we would usually like to read environment variables from an `.env` file, for our needs in this lab, we will hard code these in this cell, to make things more clear\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5dad55-f81f-482a-8080-85115078f8b8",
   "metadata": {},
   "source": [
    "### Initialize the *Client* \n",
    "We will not define our client, and introduce a new pattern/trick that will help us later when we try to move between models and other providers within Llama Stack. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193a3474-d8d9-4ea9-a205-61e5a4b46dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLAMA_STACK_SERVER=os.getenv(\"LLAMA_STACK_SERVER\")\n",
    "\n",
    "client = LlamaStackClient(\n",
    "    base_url=LLAMA_STACK_SERVER,\n",
    "    provider_data=provider_data\n",
    ")\n",
    "# List available models and select from allowed models list\n",
    "allowed_models_list=[\"meta-llama/Llama-3.2-3B-Instruct\"]\n",
    "selected_model = None\n",
    "\n",
    "models = client.models.list()\n",
    "\n",
    "print(\"--- Available models: ---\")\n",
    "for m in models:\n",
    "    print(f\"{m.identifier} - {m.provider_id} - {m.provider_resource_id}\")\n",
    "    # Check if the model identifier contains any of the allowed substrings\n",
    "    if any(substring in m.identifier for substring in allowed_models_list):\n",
    "        # Only set selected_model if it hasn't been set yet\n",
    "        if selected_model is None:\n",
    "            selected_model = m.identifier\n",
    "           \n",
    "# If no allowed model was found, you might want to handle that case\n",
    "if selected_model is None:\n",
    "    print(\"No allowed model found in the list.\")\n",
    "\n",
    "\n",
    "print(f\"Selected model (from allowed list): {selected_model}\")\n",
    "            # Removed the break here to show all available models, but the selection logic remains picking the first one\n",
    "\n",
    "\n",
    "SELECTED_MODEL = selected_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621db20f-e3ba-42c1-a3e0-a6c32abc6c31",
   "metadata": {},
   "source": [
    "### Define the a single tool agent\n",
    "Here we define our new agent, we will pass along the `client` defining our connection to the Llama Stack server, the `model` we selected in the previous step, the `tools` array with all available tools for the agent, and crucually, the prompt, named `instructions` for the agent.\n",
    "\n",
    "> **Note:**\n",
    "> You might notice that we are being very specific in our prompt, for the purpose of lab we are using rather small models and are providing a narrow path to follow, in a real world scenario, prompts and LLMs can be experimented with to produce the best results.\n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97436930",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.lib.agents.agent import Agent\n",
    "\n",
    "agent = Agent(\n",
    "    client, \n",
    "    model=SELECTED_MODEL,\n",
    "    instructions=\"\"\"You are a helpful websearch assistant. When you are asked to search the latest you must use a tool. \n",
    "            Whenever a tool is called, be sure return the response in a friendly and helpful tone.\n",
    "            \"\"\" ,\n",
    "    tools=[\"builtin::websearch\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3a2cc1-17cf-4240-a987-12dccf74ceab",
   "metadata": {},
   "source": [
    "### inspecting the Agent's response structure\n",
    "\n",
    "Let's give our agent a couple of easy questions to see how it calls our `builtin::websearch` tool.\n",
    "Notice that we are generating a new session ID for each question as convesation memory is shared throughtout the session. (More on this later!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707dd1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "       \"search the web for the latest in OpenShift?\",\n",
    "       \"search for the current weather in boston\",\n",
    "]\n",
    "\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    # Generate a new Unique Identifier for each session \n",
    "    new_uuid = uuid.uuid4()\n",
    "    session_id = agent.create_session(f\"web-session-{new_uuid}\")\n",
    "\n",
    "    cprint(f\"\\n{'='*100}\\nProcessing user query: {prompt}\\n{'='*100}\", \"blue\")\n",
    "\n",
    "    \n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38a3e9d",
   "metadata": {},
   "source": [
    "This output shows an AI agent processing a user query about \"latest news OpenShift.\"\n",
    "\n",
    "* **ðŸ“ Step 1: InferenceStep**: The agent *infers* the need to search for information and decides to use the `brave_search` tool with the query \"latest news OpenShift.\"\n",
    "* **ðŸ“ Step 2: ToolExecutionStep**: The agent *executes* the `brave_search` tool. The JSON block is the *output* from this tool, providing search results with titles, URLs, snippets, and relevance scores.\n",
    "* **ðŸ“ Step 3: InferenceStep**: The agent *processes* the search results from the tool and *infers* how to construct a final answer.\n",
    "* **ðŸ¤– Model Response**: This is the AI agent's *final generated response* to the user's query, summarizing the key information extracted from the search results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883fe33f-08e6-42bb-9e9b-575ee7a8ccf4",
   "metadata": {},
   "source": [
    "### Creating our first custom tool\n",
    "\n",
    "This code defines a custom tool called `get_location` for agent to use\n",
    "\n",
    "* **`@client_tool`**: This decorator from `llama_stack_client.lib.agents.client_tool` marks the `get_location` function as a tool that the agent can discover and use to fulfill user requests. It essentially registers this function within the agent's toolkit.\n",
    "\n",
    "* **Docstring Notes**: The text within the triple quotes (`\"\"\"Docstring goes here\"\"\"`) serves as crucial instructions for the AI agent. It explains:\n",
    "    * **What the tool does**: \"Provides the user's location upon request.\"\n",
    "    * **How it works**: Mentions using the `geocoder` library and determining location via IP address.\n",
    "    * **Input parameters**: Describes the `query` parameter.\n",
    "    * **Output**: Specifies what the tool returns (location information as a string).\n",
    "    * **Example**: Shows how the tool can be used and its expected output.\n",
    "\n",
    "The agent reads this docstring to understand the tool's purpose, how to call it (with what arguments), and what kind of result to expect. This allows the agent to strategically use the `get_location` tool when a user asks for their location.\n",
    "\n",
    "> **Note:**\n",
    "> This is just one way to create a custom tool, we can also import tools from libraries or use MCP Servers (Again, more on this later...) \n",
    "> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd224b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_stack_client.lib.agents.client_tool import client_tool\n",
    "\n",
    "@client_tool\n",
    "def get_location(query: str = \"location\"):\n",
    "    \"\"\"\n",
    "    Provides the user's location upon request.\n",
    "\n",
    "    This function uses the geocoder library to determine the user's location\n",
    "    based on their IP address.  It returns the city, state, and country.\n",
    "\n",
    "    :param query:  The query from the user.  Defaults to \"location\".\n",
    "    :type query: str\n",
    "    :return:  Information about the user's current location.\n",
    "    :rtype: str\n",
    "\n",
    "    Example:\n",
    "        >>> get_location(\"where am i\")\n",
    "        \"Your current location is: Some City, Some State, Some Country\"\n",
    "    \"\"\"\n",
    "    import geocoder\n",
    "    try:\n",
    "        g = geocoder.ip('me')\n",
    "        if g.ok:\n",
    "            return f\"Your current location is: {g.city}, {g.state}, {g.country}\" # can be modified to return latitude and longitude if needed\n",
    "        else:\n",
    "            return \"Unable to determine your location\"\n",
    "    except Exception as e:\n",
    "        return f\"Error getting location: {str(e)}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b711eb4-9b77-4599-ad13-18079ae7b465",
   "metadata": {},
   "source": [
    "## Prompt Chaining (Optional)\n",
    "\n",
    "Prompt chaining allows an AI agent to remember information across multiple turns within the same session. In this example, both user questions share the same `session_id`.\n",
    "\n",
    "Because they share a session, the agent will remember the answer to the first question (\"Where am I?\") when interpreting the second question (\"search for any weather-related risks in my area...\"). This enables the agent to understand \"my area\" refers to the location identified in the previous turn, leading to a more context-aware and helpful response.\n",
    "\n",
    "### Defining a Multi-tool Agent\n",
    "\n",
    "This code initializes an AI agent with specific instructions and tools. \n",
    "The `tools` parameter is a list containing elements the agent can use. Here, it includes `get_location` (a custom function defined earlier) and `\"builtin::websearch\"` (a pre-built web searching capability in Llama Stack). \n",
    "\n",
    "\n",
    "The instructions (often refered to as \"Prompt\" guide the agent on when to use each of these tools based on the user's query.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a7e0db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "agent = Agent(\n",
    "    client, \n",
    "    model=SELECTED_MODEL,\n",
    "    # instructions=\"\"\"You are a helpful assistant. \n",
    "    # When a user asks about their location, you MUST use the get_location tool. When searching for nearby places, you MUST use the websearch tool.\n",
    "    # \"\"\" ,\n",
    "    instructions=\"\"\"You are a helpful assistant. \n",
    "    When a user asks about their location, you MUST use the get_location tool. When searching for nearby places, you MUST use the brave_search tool.\n",
    "    \"\"\" ,\n",
    "    tools=[get_location, \"builtin::websearch\"],\n",
    "    #sampling_params=sampling_params\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a3aabf-8760-439f-a851-b50646ac29a3",
   "metadata": {},
   "source": [
    "### inspecting the Agent's response structure with Prompt Chaining\n",
    "\n",
    "Notice that we are **NOT** generating a new session ID for each question, this way, the convesation memory is shared throughtout the session. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4331a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompts = [\n",
    "    \"Where am I?\",\n",
    "    \"search for any weather-related risks in my area that could disrupt network connectivity or system availability?\",\n",
    "]\n",
    "\n",
    "# Generate a new Unique Identifier for the session, but *NOT* for each question.\n",
    "\n",
    "new_uuid = uuid.uuid4()\n",
    "session_id = agent.create_session(\"prompt-chaining-session-{new_uuid}\")\n",
    "\n",
    "for prompt in user_prompts:\n",
    "    print(\"\\n\"+\"=\"*50)\n",
    "    print(f\"Processing user query: {prompt}\", \"blue\")\n",
    "    print(\"=\"*50)\n",
    "    response = agent.create_turn(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            }\n",
    "        ],\n",
    "        session_id=session_id,\n",
    "        stream=stream\n",
    "    )\n",
    "\n",
    "    if stream:\n",
    "        for log in EventLogger().log(response):\n",
    "            log.print()\n",
    "    else:\n",
    "        step_printer(response.steps) # print the steps of an agent's response in a formatted way. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6369e9f7-3ca8-4ed9-9bca-6cc174319d15",
   "metadata": {},
   "source": [
    "## Lab Summary: Interacting with Agents and Tools\n",
    "\n",
    "In this lab, you took your first steps in understanding and interacting with AI agents using the Llama Stack framework and observe the benefits of Llama Stack in providing a structured and intuitive way to build intelligent agents with access to a variety of tools, both built-in and custom, and the ability to manage conversational memory for more coherent interactions. \n",
    "\n",
    "You learned how to:\n",
    "\n",
    "* **Define a basic AI agent:** You created an agent and provided it with instructions on how to behave.\n",
    "* **Utilize built-in tools:** You explored how agents can leverage pre-existing tools within Llama Stack, specifically the `websearch` tool, to gather information from the web.\n",
    "* **Observe agent behavior:** You became familiar with the concepts of \"Turns\" and \"Steps\" that an agent goes through when processing a query, including the crucial \"Tool Call\" step where the agent decides to use a tool.\n",
    "* **Create custom tools:** You experienced how to extend an agent's capabilities by defining your own specialized tool (`get_location`) using Python and decorators.\n",
    "* **Implement prompt chaining:** You explored how to maintain context across multiple user interactions within the same session, allowing the agent to remember previous information and use it to answer subsequent questions more effectively.\n",
    "\n",
    "Through these exercises, you gained a foundational understanding of how agents operate within the Llama Stack ecosystem and how they can be equipped with tools to perform specific tasks. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
